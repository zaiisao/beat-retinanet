Script started on Tue Oct 25 15:03:39 2022
[1m[7m%[27m[1m[0m                                                                                                                                                                                                                    ]7;file://anjaehuns-MacBook-Pro.local/Volumes/External%201/GitHub/beat-retinanet[0m[27m[24m[J(base) jaehoon@anjaehuns-MacBook-Pro beat-retinanet % [K[?2004hssh root@163.239.15.21 -p 30339[31Dgit push --set-upstream origin feature/add-theme-and-start-page[54D                                                      [55Dcommit -m "Set up page system"[30Dpush                          [26D --set-upstream origin feature/add-theme-and-start-page[63Dssh root@163.239.15.21 -p 30339                                [32D[31D                               [31D
bck-i-search: _[K[A[39Cgit [4mb[24mranch[1B[50Db_[A[42C[4mb[4mr[24m[1B[45Dr_[A[41Cpyth[24mo[24mn train.py --ballroom_audio_dir ../../beat-tracking-dataset/labeled_data/train/br_test/data --ballroom_annot_dir ../../beat-tracking-dataset/labeled_data/train/[4mb[4mr[4m_[24mtest/label --preload --patience 10 --train_length 2097152 --eval_length 2097152 --act_type PReLU --norm_type BatchNorm --channel_width 32 --channel_growth 32 --augment --batch_size 2 --audio_sample_rate 22050 --num_workers 0[K
bck-i-search: br__[K[A[A[11D[24mb[24mr[24m_[2B[K[A[A[7C[?2004l[2B/bin/sh: sox: command not found
SoX could not be found!

    If you do not have SoX, proceed here:
     - - - http://sox.sourceforge.net/ - - -

    If you do (or think that you should) have SoX, double-check your
    path variables.
    
lightning_logs/full
no checkpoint found
Selected 7 files for train set from ballroom dataset.
  0%|                                                     | 0/7 [00:00<?, ?it/s] 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                      | 1/7 [00:00<00:01,  3.76it/s] 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 2/7 [00:00<00:01,  3.76it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 3/7 [00:00<00:00,  5.00it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 4/7 [00:00<00:00,  5.87it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 5/7 [00:00<00:00,  6.53it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 6/7 [00:01<00:00,  7.12it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  7.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  6.15it/s]
Selected 1 files for val set from ballroom dataset.
  0%|                                                     | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.11it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.10it/s]
Num training images: 1000
torch.stack(classification_losses) shape in focal loss forward: torch.Size([2])
torch.stack(classification_losses) in focal loss forward: tensor([1.1284, 1.1284], grad_fn=<StackBackward0>)
torch.stack(classification_losses).mean(dim=0, keepdim=True) in focal loss forward: tensor([1.1284], grad_fn=<MeanBackward1>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([2855])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([2855])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 2855])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([2855, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([2855, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([2855, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 3.9337,  1.8183],
        [ 4.6729,  2.9736],
        [ 3.7089,  1.8183],
        ...,
        [-4.5312,  2.7335],
        [-3.5965,  1.5782],
        [-3.8212,  1.5782]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[3.8781, 1.7628],
        [4.6173, 2.9180],
        [3.6533, 1.7628],
        ...,
        [4.4757, 2.6779],
        [3.5409, 1.5227],
        [3.7657, 1.5227]], grad_fn=<SWhereBackward0>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([4646])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([4646])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 4646])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([4646, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([4646, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([4646, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 2.9221, -0.6583],
        [ 3.3985,  0.4970],
        [ 2.6974, -0.6583],
        ...,
        [-3.3984,  0.4969],
        [-2.6973, -0.6583],
        [-2.9221, -0.6583]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[2.8666, 0.6027],
        [3.3429, 0.4414],
        [2.6418, 0.6027],
        ...,
        [3.3429, 0.4413],
        [2.6418, 0.6028],
        [2.8666, 0.6028]], grad_fn=<SWhereBackward0>)
feature index 0
cls loss:
 tensor([1.1284], grad_fn=<MeanBackward1>)
 reg loss: tensor([1.6554], grad_fn=<MeanBackward1>)

torch.stack(classification_losses) shape in focal loss forward: torch.Size([2])
torch.stack(classification_losses) in focal loss forward: tensor([1.1284, 1.1284], grad_fn=<StackBackward0>)
torch.stack(classification_losses).mean(dim=0, keepdim=True) in focal loss forward: tensor([1.1284], grad_fn=<MeanBackward1>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([2288])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([2288])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 2288])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([2288, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([2288, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([2288, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 3.0759, -0.0792],
        [ 3.4878,  1.0760],
        [ 2.7683, -0.0792],
        ...,
        [-3.3910,  0.8360],
        [-2.6914, -0.3192],
        [-2.9990, -0.3192]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[3.0203, 0.0282],
        [3.4323, 1.0205],
        [2.7127, 0.0282],
        ...,
        [3.3354, 0.7805],
        [2.6359, 0.2637],
        [2.9435, 0.2637]], grad_fn=<SWhereBackward0>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([2820])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([2820])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 2820])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([2820, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([2820, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([2820, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 2.5376, -2.5558],
        [ 2.8096, -1.4006],
        [ 2.2300, -2.5558],
        ...,
        [-2.8097, -1.4005],
        [-2.2300, -2.5558],
        [-2.5376, -2.5558]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[2.4820, 2.5003],
        [2.7541, 1.3450],
        [2.1745, 2.5003],
        ...,
        [2.7541, 1.3450],
        [2.1745, 2.5002],
        [2.4821, 2.5002]], grad_fn=<SWhereBackward0>)
feature index 1
cls loss:
 tensor([1.1284], grad_fn=<MeanBackward1>)
 reg loss: tensor([1.3746], grad_fn=<MeanBackward1>)

torch.stack(classification_losses) shape in focal loss forward: torch.Size([2])
torch.stack(classification_losses) in focal loss forward: tensor([1.1284, 1.1284], grad_fn=<StackBackward0>)
torch.stack(classification_losses).mean(dim=0, keepdim=True) in focal loss forward: tensor([1.1284], grad_fn=<MeanBackward1>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([1369])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([1369])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 1369])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([1369, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([1369, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([1369, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 2.8309, -1.6099],
        [ 2.9961, -0.4546],
        [ 2.3780, -1.6099],
        ...,
        [-2.6394, -0.6947],
        [-2.0949, -1.8499],
        [-2.5478, -1.8499]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[2.7754, 1.5543],
        [2.9405, 0.3991],
        [2.3224, 1.5543],
        ...,
        [2.5838, 0.6391],
        [2.0393, 1.7944],
        [2.4923, 1.7944]], grad_fn=<SWhereBackward0>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([1362])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([1362])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 1362])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([1362, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([1362, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([1362, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 2.4967, -2.9312],
        [ 2.4267, -1.7760],
        [ 1.9260, -2.9312],
        ...,
        [-1.6407, -2.9312],
        [-2.7862, -1.7760],
        [-2.2114, -2.9312]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[2.4412, 2.8757],
        [2.3711, 1.7204],
        [1.8705, 2.8757],
        ...,
        [1.5852, 2.8757],
        [2.7306, 1.7204],
        [2.1558, 2.8757]], grad_fn=<SWhereBackward0>)
feature index 2
cls loss:
 tensor([1.1284], grad_fn=<MeanBackward1>)
 reg loss: tensor([1.6491], grad_fn=<MeanBackward1>)

torch.stack(classification_losses) shape in focal loss forward: torch.Size([2])
torch.stack(classification_losses) in focal loss forward: tensor([1.1284, 1.1284], grad_fn=<StackBackward0>)
torch.stack(classification_losses).mean(dim=0, keepdim=True) in focal loss forward: tensor([1.1284], grad_fn=<MeanBackward1>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([654])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([654])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 654])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([654, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([654, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([654, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 2.6819, -2.2651],
        [ 2.1286, -3.4203],
        [ 1.8873, -2.2651],
        ...,
        [-0.8443, -2.5051],
        [-1.6389, -2.5051],
        [-2.4336, -2.5051]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[2.6263, 2.2095],
        [2.0731, 3.3648],
        [1.8317, 2.2095],
        ...,
        [0.7887, 2.4496],
        [1.5834, 2.4496],
        [2.3780, 2.4496]], grad_fn=<SWhereBackward0>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([533])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([533])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 533])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([533, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([533, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([533, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 3.6265,  1.0345],
        [ 2.8784, -0.1207],
        [ 3.7745,  2.1898],
        ...,
        [-2.2839, -0.1529],
        [-3.5083,  1.0024],
        [-2.7845, -0.1529]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[3.5710, 0.9790],
        [2.8228, 0.0651],
        [3.7190, 2.1342],
        ...,
        [2.2284, 0.0973],
        [3.4527, 0.9468],
        [2.7290, 0.0973]], grad_fn=<SWhereBackward0>)
feature index 3
cls loss:
 tensor([1.1284], grad_fn=<MeanBackward1>)
 reg loss: tensor([1.5523], grad_fn=<MeanBackward1>)

torch.stack(classification_losses) shape in focal loss forward: torch.Size([2])
torch.stack(classification_losses) in focal loss forward: tensor([1.1284, 1.1284], grad_fn=<StackBackward0>)
torch.stack(classification_losses).mean(dim=0, keepdim=True) in focal loss forward: tensor([1.1284], grad_fn=<MeanBackward1>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([341])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([341])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 341])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([341, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([341, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([341, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 2.8222, -1.2639],
        [ 2.2400, -2.4191],
        [ 2.7003, -0.1086],
        [ 2.1432, -1.2639],
        [ 1.7011, -2.4191],
        [ 1.8447, -0.1086],
        [ 1.4642, -1.2639],
        [ 1.1621, -2.4191],
        [ 0.9892, -0.1086],
        [ 0.7851, -1.2639],
        [ 0.6232, -2.4191],
        [ 0.1337, -0.1086],
        [ 0.1061, -1.2639],
        [ 0.0842, -2.4191],
        [-0.7218, -0.1086],
        [-0.5729, -1.2639],
        [-0.4547, -2.4191],
        [-1.5774, -0.1086],
        [-1.2520, -1.2639],
        [-0.9937, -2.4191],
        [-2.4329, -0.1086],
        [-1.9310, -1.2639],
        [-1.5326, -2.4191],
        [-3.2884, -0.1086],
        [-2.6100, -1.2639],
        [-2.0716, -2.4191],
        [-2.6105, -2.4191],
        [ 2.4084, -2.5296],
        [ 2.9676, -0.2191],
        [ 2.3554, -1.3744],
        [ 1.8695, -2.5296],
        [ 2.1121, -0.2191],
        [ 1.6764, -1.3744],
        [ 1.3305, -2.5296],
        [ 1.2566, -0.2191],
        [ 0.9973, -1.3744],
        [ 0.7916, -2.5296],
        [ 0.4010, -0.2191],
        [ 0.3183, -1.3744],
        [ 0.2526, -2.5296],
        [-0.4545, -0.2191],
        [-0.3607, -1.3744],
        [-0.2863, -2.5296],
        [-1.3100, -0.2191],
        [-1.0398, -1.3744],
        [-0.8253, -2.5296],
        [-2.1655, -0.2191],
        [-1.7188, -1.3744],
        [-1.3642, -2.5296],
        [-3.0211, -0.2191],
        [-2.3978, -1.3744],
        [-1.9032, -2.5296],
        [-2.4421, -2.5296],
        [ 2.5263, -2.5018],
        [ 3.1548, -0.1913],
        [ 2.5039, -1.3465],
        [ 1.9874, -2.5018],
        [ 2.2992, -0.1913],
        [ 1.8249, -1.3465],
        [ 1.4484, -2.5018],
        [ 1.4437, -0.1913],
        [ 1.1459, -1.3465],
        [ 0.9095, -2.5018],
        [ 0.5882, -0.1913],
        [ 0.4668, -1.3465],
        [ 0.3705, -2.5018],
        [-0.2674, -0.1913],
        [-0.2122, -1.3465],
        [-0.1684, -2.5018],
        [-1.1229, -0.1913],
        [-0.8912, -1.3465],
        [-0.7074, -2.5018],
        [-1.9784, -0.1913],
        [-1.5703, -1.3465],
        [-1.2463, -2.5018],
        [-2.8339, -0.1913],
        [-2.2493, -1.3465],
        [-1.7853, -2.5018],
        [-2.9283, -1.3465],
        [-2.3242, -2.5018],
        [ 2.5768, -2.6426],
        [ 2.5676, -1.4874],
        [ 2.0379, -2.6426],
        [ 2.3794, -0.3321],
        [ 1.8886, -1.4874],
        [ 1.4989, -2.6426],
        [ 1.5239, -0.3321],
        [ 1.2095, -1.4874],
        [ 0.9600, -2.6426],
        [ 0.6684, -0.3321],
        [ 0.5305, -1.4874],
        [ 0.4211, -2.6426],
        [-0.1871, -0.3321],
        [-0.1485, -1.4874],
        [-0.1179, -2.6426],
        [-1.0427, -0.3321],
        [-0.8276, -1.4874],
        [-0.6568, -2.6426],
        [-1.8982, -0.3321],
        [-1.5066, -1.4874],
        [-1.1958, -2.6426],
        [-2.7537, -0.3321],
        [-2.1856, -1.4874],
        [-1.7347, -2.6426],
        [-2.8647, -1.4874],
        [-2.2737, -2.6426],
        [ 2.6442, -2.4741],
        [ 2.6525, -1.3188],
        [ 2.1053, -2.4741],
        [ 2.4864, -0.1636],
        [ 1.9734, -1.3188],
        [ 1.5663, -2.4741],
        [ 1.6308, -0.1636],
        [ 1.2944, -1.3188],
        [ 1.0274, -2.4741],
        [ 0.7753, -0.1636],
        [ 0.6154, -1.3188],
        [ 0.4884, -2.4741],
        [-0.0802, -0.1636],
        [-0.0637, -1.3188],
        [-0.0505, -2.4741],
        [-0.9357, -0.1636],
        [-0.7427, -1.3188],
        [-0.5895, -2.4741],
        [-1.7913, -0.1636],
        [-1.4217, -1.3188],
        [-1.1284, -2.4741],
        [-2.6468, -0.1636],
        [-2.1008, -1.3188],
        [-1.6674, -2.4741],
        [-2.7798, -1.3188],
        [-2.2063, -2.4741],
        [ 2.8434, -1.3465],
        [ 2.2568, -2.5018],
        [ 2.7270, -0.1913],
        [ 2.1644, -1.3465],
        [ 1.7179, -2.5018],
        [ 1.8715, -0.1913],
        [ 1.4854, -1.3465],
        [ 1.1789, -2.5018],
        [ 1.0159, -0.1913],
        [ 0.8064, -1.3465],
        [ 0.6400, -2.5018],
        [ 0.1604, -0.1913],
        [ 0.1273, -1.3465],
        [ 0.1011, -2.5018],
        [-0.6951, -0.1913],
        [-0.5517, -1.3465],
        [-0.4379, -2.5018],
        [-1.5506, -0.1913],
        [-1.2307, -1.3465],
        [-0.9768, -2.5018],
        [-2.4062, -0.1913],
        [-1.9098, -1.3465],
        [-1.5158, -2.5018],
        [-3.2617, -0.1913],
        [-2.5888, -1.3465],
        [-2.0547, -2.5018],
        [-2.5937, -2.5018],
        [ 2.3074, -2.6426],
        [ 2.8072, -0.3321],
        [ 2.2281, -1.4874],
        [ 1.7684, -2.6426],
        [ 1.9517, -0.3321],
        [ 1.5490, -1.4874],
        [ 1.2295, -2.6426],
        [ 1.0961, -0.3321],
        [ 0.8700, -1.4874],
        [ 0.6905, -2.6426],
        [ 0.2406, -0.3321],
        [ 0.1910, -1.4874],
        [ 0.1516, -2.6426],
        [-0.6149, -0.3321],
        [-0.4881, -1.4874],
        [-0.3874, -2.6426],
        [-1.4704, -0.3321],
        [-1.1671, -1.4874],
        [-0.9263, -2.6426],
        [-2.3260, -0.3321],
        [-1.8461, -1.4874],
        [-1.4653, -2.6426],
        [-3.1815, -0.3321],
        [-2.5251, -1.4874],
        [-2.0042, -2.6426],
        [-2.5432, -2.6426],
        [ 2.4084, -2.4191],
        [ 2.9676, -0.1086],
        [ 2.3554, -1.2639],
        [ 1.8695, -2.4191],
        [ 2.1121, -0.1086],
        [ 1.6764, -1.2639],
        [ 1.3305, -2.4191],
        [ 1.2566, -0.1086],
        [ 0.9973, -1.2639],
        [ 0.7916, -2.4191],
        [ 0.4010, -0.1086],
        [ 0.3183, -1.2639],
        [ 0.2526, -2.4191],
        [-0.4545, -0.1086],
        [-0.3607, -1.2639],
        [-0.2863, -2.4191],
        [-1.3100, -0.1086],
        [-1.0398, -1.2639],
        [-0.8253, -2.4191],
        [-2.1655, -0.1086],
        [-1.7188, -1.2639],
        [-1.3642, -2.4191],
        [-3.0211, -0.1086],
        [-2.3978, -1.2639],
        [-1.9032, -2.4191],
        [-2.4421, -2.4191],
        [ 2.5263, -2.6141],
        [ 3.1548, -0.3036],
        [ 2.5039, -1.4589],
        [ 1.9874, -2.6141],
        [ 2.2992, -0.3036],
        [ 1.8249, -1.4589],
        [ 1.4484, -2.6141],
        [ 1.4437, -0.3036],
        [ 1.1459, -1.4589],
        [ 0.9095, -2.6141],
        [ 0.5882, -0.3036],
        [ 0.4668, -1.4589],
        [ 0.3705, -2.6141],
        [-0.2674, -0.3036],
        [-0.2122, -1.4589],
        [-0.1684, -2.6141],
        [-1.1229, -0.3036],
        [-0.8912, -1.4589],
        [-0.7074, -2.6141],
        [-1.9784, -0.3036],
        [-1.5703, -1.4589],
        [-1.2463, -2.6141],
        [-2.8339, -0.3036],
        [-2.2493, -1.4589],
        [-1.7853, -2.6141],
        [-2.3242, -2.6141],
        [ 2.6105, -2.4741],
        [ 2.6100, -1.3188],
        [ 2.0716, -2.4741],
        [ 2.4329, -0.1636],
        [ 1.9310, -1.3188],
        [ 1.5326, -2.4741],
        [ 1.5774, -0.1636],
        [ 1.2520, -1.3188],
        [ 0.9937, -2.4741],
        [ 0.7218, -0.1636],
        [ 0.5729, -1.3188],
        [ 0.4547, -2.4741],
        [-0.1337, -0.1636],
        [-0.1061, -1.3188],
        [-0.0842, -2.4741],
        [-0.9892, -0.1636],
        [-0.7851, -1.3188],
        [-0.6232, -2.4741],
        [-1.8447, -0.1636],
        [-1.4642, -1.3188],
        [-1.1621, -2.4741],
        [-2.7003, -0.1636],
        [-2.1432, -1.3188],
        [-1.7011, -2.4741],
        [-2.8222, -1.3188],
        [-2.2400, -2.4741],
        [ 2.7161, -1.4589],
        [ 2.1558, -2.6141],
        [ 2.5666, -0.3036],
        [ 2.0371, -1.4589],
        [ 1.6168, -2.6141],
        [ 1.7111, -0.3036],
        [ 1.3581, -1.4589],
        [ 1.0779, -2.6141],
        [ 0.8555, -0.3036],
        [ 0.6790, -1.4589],
        [ 0.5389, -2.6141],
        [ 0.0000, -0.3036],
        [ 0.0000, -1.4589],
        [ 0.0000, -2.6141],
        [-0.8555, -0.3036],
        [-0.6790, -1.4589],
        [-0.5389, -2.6141],
        [-1.7111, -0.3036],
        [-1.3581, -1.4589],
        [-1.0779, -2.6141],
        [-2.5666, -0.3036],
        [-2.0371, -1.4589],
        [-1.6168, -2.6141],
        [-2.7161, -1.4589],
        [-2.1558, -2.6141],
        [ 2.5937, -2.7876],
        [ 2.5888, -1.6323],
        [ 2.0547, -2.7876],
        [ 2.4062, -0.4771],
        [ 1.9098, -1.6323],
        [ 1.5158, -2.7876],
        [ 1.5506, -0.4771],
        [ 1.2307, -1.6323],
        [ 0.9768, -2.7876],
        [ 0.6951, -0.4771],
        [ 0.5517, -1.6323],
        [ 0.4379, -2.7876],
        [-0.1604, -0.4771],
        [-0.1273, -1.6323],
        [-0.1011, -2.7876],
        [-1.0159, -0.4771],
        [-0.8063, -1.6323],
        [-0.6400, -2.7876],
        [-1.8715, -0.4771],
        [-1.4854, -1.6323],
        [-1.1789, -2.7876],
        [-2.7270, -0.4771],
        [-2.1644, -1.6323],
        [-1.7179, -2.7876],
        [-2.8434, -1.6323],
        [-2.2568, -2.7876],
        [ 2.5937, -2.4465],
        [ 3.2617, -0.1360],
        [ 2.5888, -1.2913],
        [ 2.0547, -2.4465],
        [ 2.4062, -0.1360],
        [ 1.9098, -1.2913],
        [ 1.5158, -2.4465],
        [ 1.5506, -0.1360],
        [ 1.2307, -1.2913],
        [ 0.9768, -2.4465],
        [ 0.6951, -0.1360],
        [ 0.5517, -1.2913],
        [ 0.4379, -2.4465],
        [-0.1604, -0.1360],
        [-0.1273, -1.2913],
        [-0.1011, -2.4465],
        [-1.0159, -0.1360],
        [-0.8063, -1.2913],
        [-0.6400, -2.4465],
        [-1.8715, -0.1360],
        [-1.4854, -1.2913],
        [-1.1789, -2.4465],
        [-2.7270, -0.1360],
        [-2.1644, -1.2913],
        [-1.7179, -2.4465],
        [-2.8434, -1.2913],
        [-2.2568, -2.4465]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[2.7667, 1.2083],
        [2.1844, 2.3636],
        [2.6447, 0.0531],
        [2.0876, 1.2083],
        [1.6455, 2.3636],
        [1.7892, 0.0531],
        [1.4086, 1.2083],
        [1.1066, 2.3636],
        [0.9336, 0.0531],
        [0.7296, 1.2083],
        [0.5676, 2.3636],
        [0.0781, 0.0531],
        [0.0507, 1.2083],
        [0.0319, 2.3636],
        [0.6663, 0.0531],
        [0.5174, 1.2083],
        [0.3992, 2.3636],
        [1.5218, 0.0531],
        [1.1964, 1.2083],
        [0.9381, 2.3636],
        [2.3773, 0.0531],
        [1.8754, 1.2083],
        [1.4771, 2.3636],
        [3.2329, 0.0531],
        [2.5545, 1.2083],
        [2.0160, 2.3636],
        [2.5550, 2.3636],
        [2.3529, 2.4741],
        [2.9120, 0.1636],
        [2.2998, 1.3188],
        [1.8139, 2.4741],
        [2.0565, 0.1636],
        [1.6208, 1.3188],
        [1.2750, 2.4741],
        [1.2010, 0.1636],
        [0.9418, 1.3188],
        [0.7360, 2.4741],
        [0.3455, 0.1636],
        [0.2627, 1.3188],
        [0.1971, 2.4741],
        [0.3989, 0.1636],
        [0.3052, 1.3188],
        [0.2308, 2.4741],
        [1.2545, 0.1636],
        [0.9842, 1.3188],
        [0.7697, 2.4741],
        [2.1100, 0.1636],
        [1.6632, 1.3188],
        [1.3087, 2.4741],
        [2.9655, 0.1636],
        [2.3423, 1.3188],
        [1.8476, 2.4741],
        [2.3866, 2.4741],
        [2.4708, 2.4462],
        [3.0992, 0.1357],
        [2.4484, 1.2910],
        [1.9318, 2.4462],
        [2.2437, 0.1357],
        [1.7693, 1.2910],
        [1.3929, 2.4462],
        [1.3881, 0.1357],
        [1.0903, 1.2910],
        [0.8539, 2.4462],
        [0.5326, 0.1357],
        [0.4113, 1.2910],
        [0.3150, 2.4462],
        [0.2118, 0.1357],
        [0.1566, 1.2910],
        [0.1129, 2.4462],
        [1.0673, 0.1357],
        [0.8357, 1.2910],
        [0.6518, 2.4462],
        [1.9228, 0.1357],
        [1.5147, 1.2910],
        [1.1908, 2.4462],
        [2.7784, 0.1357],
        [2.1937, 1.2910],
        [1.7297, 2.4462],
        [2.8728, 1.2910],
        [2.2687, 2.4462],
        [2.5213, 2.5871],
        [2.5120, 1.4318],
        [1.9823, 2.5871],
        [2.3239, 0.2766],
        [1.8330, 1.4318],
        [1.4434, 2.5871],
        [1.4683, 0.2766],
        [1.1540, 1.4318],
        [0.9044, 2.5871],
        [0.6128, 0.2766],
        [0.4749, 1.4318],
        [0.3655, 2.5871],
        [0.1316, 0.2766],
        [0.0930, 1.4318],
        [0.0623, 2.5871],
        [0.9871, 0.2766],
        [0.7720, 1.4318],
        [0.6013, 2.5871],
        [1.8426, 0.2766],
        [1.4510, 1.4318],
        [1.1402, 2.5871],
        [2.6982, 0.2766],
        [2.1301, 1.4318],
        [1.6792, 2.5871],
        [2.8091, 1.4318],
        [2.2181, 2.5871],
        [2.5887, 2.4185],
        [2.5969, 1.2633],
        [2.0497, 2.4185],
        [2.4308, 0.1080],
        [1.9179, 1.2633],
        [1.5108, 2.4185],
        [1.5753, 0.1080],
        [1.2388, 1.2633],
        [0.9718, 2.4185],
        [0.7198, 0.1080],
        [0.5598, 1.2633],
        [0.4329, 2.4185],
        [0.0289, 0.1080],
        [0.0182, 1.2633],
        [0.0115, 2.4185],
        [0.8802, 0.1080],
        [0.6871, 1.2633],
        [0.5339, 2.4185],
        [1.7357, 0.1080],
        [1.3662, 1.2633],
        [1.0729, 2.4185],
        [2.5912, 0.1080],
        [2.0452, 1.2633],
        [1.6118, 2.4185],
        [2.7242, 1.2633],
        [2.1508, 2.4185],
        [2.7879, 1.2910],
        [2.2013, 2.4462],
        [2.6714, 0.1357],
        [2.1089, 1.2910],
        [1.6623, 2.4462],
        [1.8159, 0.1357],
        [1.4298, 1.2910],
        [1.1234, 2.4462],
        [0.9604, 0.1357],
        [0.7508, 1.2910],
        [0.5844, 2.4462],
        [0.1049, 0.1357],
        [0.0718, 1.2910],
        [0.0460, 2.4462],
        [0.6396, 0.1357],
        [0.4962, 1.2910],
        [0.3823, 2.4462],
        [1.4951, 0.1357],
        [1.1752, 1.2910],
        [0.9213, 2.4462],
        [2.3506, 0.1357],
        [1.8542, 1.2910],
        [1.4602, 2.4462],
        [3.2061, 0.1357],
        [2.5333, 1.2910],
        [1.9992, 2.4462],
        [2.5381, 2.4462],
        [2.2518, 2.5871],
        [2.7516, 0.2766],
        [2.1725, 1.4318],
        [1.7129, 2.5871],
        [1.8961, 0.2766],
        [1.4935, 1.4318],
        [1.1739, 2.5871],
        [1.0406, 0.2766],
        [0.8145, 1.4318],
        [0.6350, 2.5871],
        [0.1851, 0.2766],
        [0.1354, 1.4318],
        [0.0960, 2.5871],
        [0.5594, 0.2766],
        [0.4325, 1.4318],
        [0.3318, 2.5871],
        [1.4149, 0.2766],
        [1.1115, 1.4318],
        [0.8708, 2.5871],
        [2.2704, 0.2766],
        [1.7906, 1.4318],
        [1.4097, 2.5871],
        [3.1259, 0.2766],
        [2.4696, 1.4318],
        [1.9487, 2.5871],
        [2.4876, 2.5871],
        [2.3529, 2.3636],
        [2.9120, 0.0531],
        [2.2998, 1.2083],
        [1.8139, 2.3636],
        [2.0565, 0.0531],
        [1.6208, 1.2083],
        [1.2750, 2.3636],
        [1.2010, 0.0531],
        [0.9418, 1.2083],
        [0.7360, 2.3636],
        [0.3455, 0.0531],
        [0.2627, 1.2083],
        [0.1971, 2.3636],
        [0.3989, 0.0531],
        [0.3052, 1.2083],
        [0.2308, 2.3636],
        [1.2545, 0.0531],
        [0.9842, 1.2083],
        [0.7697, 2.3636],
        [2.1100, 0.0531],
        [1.6632, 1.2083],
        [1.3087, 2.3636],
        [2.9655, 0.0531],
        [2.3423, 1.2083],
        [1.8476, 2.3636],
        [2.3865, 2.3636],
        [2.4708, 2.5586],
        [3.0992, 0.2481],
        [2.4484, 1.4033],
        [1.9318, 2.5586],
        [2.2437, 0.2481],
        [1.7693, 1.4033],
        [1.3929, 2.5586],
        [1.3881, 0.2481],
        [1.0903, 1.4033],
        [0.8539, 2.5586],
        [0.5326, 0.2481],
        [0.4113, 1.4033],
        [0.3150, 2.5586],
        [0.2118, 0.2481],
        [0.1566, 1.4033],
        [0.1129, 2.5586],
        [1.0673, 0.2481],
        [0.8357, 1.4033],
        [0.6518, 2.5586],
        [1.9228, 0.2481],
        [1.5147, 1.4033],
        [1.1908, 2.5586],
        [2.7784, 0.2481],
        [2.1937, 1.4033],
        [1.7297, 2.5586],
        [2.2687, 2.5586],
        [2.5550, 2.4185],
        [2.5545, 1.2633],
        [2.0160, 2.4185],
        [2.3773, 0.1080],
        [1.8754, 1.2633],
        [1.4771, 2.4185],
        [1.5218, 0.1080],
        [1.1964, 1.2633],
        [0.9381, 2.4185],
        [0.6663, 0.1080],
        [0.5174, 1.2633],
        [0.3992, 2.4185],
        [0.0781, 0.1080],
        [0.0507, 1.2633],
        [0.0319, 2.4185],
        [0.9336, 0.1080],
        [0.7296, 1.2633],
        [0.5676, 2.4185],
        [1.7892, 0.1080],
        [1.4086, 1.2633],
        [1.1065, 2.4185],
        [2.6447, 0.1080],
        [2.0876, 1.2633],
        [1.6455, 2.4185],
        [2.7667, 1.2633],
        [2.1844, 2.4185],
        [2.6606, 1.4033],
        [2.1002, 2.5586],
        [2.5110, 0.2481],
        [1.9815, 1.4033],
        [1.5613, 2.5586],
        [1.6555, 0.2481],
        [1.3025, 1.4033],
        [1.0223, 2.5586],
        [0.8000, 0.2481],
        [0.6235, 1.4033],
        [0.4834, 2.5586],
        [0.0000, 0.2481],
        [0.0000, 1.4033],
        [0.0000, 2.5586],
        [0.8000, 0.2481],
        [0.6235, 1.4033],
        [0.4834, 2.5586],
        [1.6555, 0.2481],
        [1.3025, 1.4033],
        [1.0223, 2.5586],
        [2.5110, 0.2481],
        [1.9815, 1.4033],
        [1.5613, 2.5586],
        [2.6606, 1.4033],
        [2.1002, 2.5586],
        [2.5381, 2.7320],
        [2.5333, 1.5768],
        [1.9992, 2.7320],
        [2.3506, 0.4215],
        [1.8542, 1.5768],
        [1.4602, 2.7320],
        [1.4951, 0.4215],
        [1.1752, 1.5768],
        [0.9213, 2.7320],
        [0.6396, 0.4215],
        [0.4962, 1.5768],
        [0.3823, 2.7320],
        [0.1049, 0.4215],
        [0.0718, 1.5768],
        [0.0460, 2.7320],
        [0.9604, 0.4215],
        [0.7508, 1.5768],
        [0.5844, 2.7320],
        [1.8159, 0.4215],
        [1.4298, 1.5768],
        [1.1234, 2.7320],
        [2.6714, 0.4215],
        [2.1089, 1.5768],
        [1.6623, 2.7320],
        [2.7879, 1.5768],
        [2.2013, 2.7320],
        [2.5381, 2.3910],
        [3.2061, 0.0805],
        [2.5333, 1.2357],
        [1.9992, 2.3910],
        [2.3506, 0.0805],
        [1.8542, 1.2357],
        [1.4602, 2.3910],
        [1.4951, 0.0805],
        [1.1752, 1.2357],
        [0.9213, 2.3910],
        [0.6396, 0.0805],
        [0.4962, 1.2357],
        [0.3823, 2.3910],
        [0.1049, 0.0805],
        [0.0718, 1.2357],
        [0.0460, 2.3910],
        [0.9604, 0.0805],
        [0.7508, 1.2357],
        [0.5844, 2.3910],
        [1.8159, 0.0805],
        [1.4298, 1.2357],
        [1.1234, 2.3910],
        [2.6714, 0.0805],
        [2.1089, 1.2357],
        [1.6623, 2.3910],
        [2.7879, 1.2357],
        [2.2013, 2.3910]], grad_fn=<SWhereBackward0>)
epoch: 0, iter: 0 targets_dx shape:
torch.Size([371])
epoch: 0, iter: 0 targets_dw shape:
torch.Size([371])
epoch: 0, iter: 0 targets.shape before t():
torch.Size([2, 371])
epoch: 0, iter: 0 targets.shape after t():
torch.Size([371, 2])
epoch: 0, iter: 0 targets shape:
torch.Size([371, 2])
epoch: 0, iter: 0 jth_regression[positive_indices, :] shape:
torch.Size([371, 2])
epoch: 0, iter: 0 regression prediction in regressionLoss forward:
 tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.],
        [0., 0.]], grad_fn=<IndexBackward0>)
epoch: 0, iter: 0 regression targets in regressionLoss forward:
 tensor([[ 2.2232, -3.2173],
        [ 2.6735, -0.9068],
        [ 2.1220, -2.0620],
        [ 1.6842, -3.2173],
        [ 1.8180, -0.9068],
        [ 1.4429, -2.0620],
        [ 1.1453, -3.2173],
        [ 0.9625, -0.9068],
        [ 0.7639, -2.0620],
        [ 0.6063, -3.2173],
        [ 0.1069, -0.9068],
        [ 0.0849, -2.0620],
        [ 0.0674, -3.2173],
        [-0.7486, -0.9068],
        [-0.5942, -2.0620],
        [-0.4716, -3.2173],
        [-1.6041, -0.9068],
        [-1.2732, -2.0620],
        [-1.0105, -3.2173],
        [-2.4596, -0.9068],
        [-1.9522, -2.0620],
        [-1.5495, -3.2173],
        [-2.6313, -2.0620],
        [-2.0884, -3.2173],
        [ 2.6525, -2.0301],
        [ 2.1053, -3.1853],
        [ 2.4864, -0.8748],
        [ 1.9734, -2.0301],
        [ 1.5663, -3.1853],
        [ 1.6308, -0.8748],
        [ 1.2944, -2.0301],
        [ 1.0274, -3.1853],
        [ 0.7753, -0.8748],
        [ 0.6154, -2.0301],
        [ 0.4884, -3.1853],
        [-0.0802, -0.8748],
        [-0.0637, -2.0301],
        [-0.0505, -3.1853],
        [-0.9357, -0.8748],
        [-0.7427, -2.0301],
        [-0.5895, -3.1853],
        [-1.7913, -0.8748],
        [-1.4217, -2.0301],
        [-1.1284, -3.1853],
        [-2.6468, -0.8748],
        [-2.1008, -2.0301],
        [-1.6674, -3.1853],
        [-2.2063, -3.1853],
        [ 2.5095, -3.2494],
        [ 2.4827, -2.0942],
        [ 1.9705, -3.2494],
        [ 2.2725, -0.9389],
        [ 1.8037, -2.0942],
        [ 1.4316, -3.2494],
        [ 1.4170, -0.9389],
        [ 1.1246, -2.0942],
        [ 0.8926, -3.2494],
        [ 0.5614, -0.9389],
        [ 0.4456, -2.0942],
        [ 0.3537, -3.2494],
        [-0.2941, -0.9389],
        [-0.2334, -2.0942],
        [-0.1853, -3.2494],
        [-1.1496, -0.9389],
        [-0.9124, -2.0942],
        [-0.7242, -3.2494],
        [-2.0051, -0.9389],
        [-1.5915, -2.0942],
        [-1.2632, -3.2494],
        [-2.8607, -0.9389],
        [-2.2705, -2.0942],
        [-1.8021, -3.2494],
        [-2.3411, -3.2494],
        [ 2.3579, -3.2173],
        [ 2.8874, -0.9068],
        [ 2.2917, -2.0620],
        [ 1.8190, -3.2173],
        [ 2.0319, -0.9068],
        [ 1.6127, -2.0620],
        [ 1.2800, -3.2173],
        [ 1.1763, -0.9068],
        [ 0.9337, -2.0620],
        [ 0.7411, -3.2173],
        [ 0.3208, -0.9068],
        [ 0.2546, -2.0620],
        [ 0.2021, -3.2173],
        [-0.5347, -0.9068],
        [-0.4244, -2.0620],
        [-0.3368, -3.2173],
        [-1.3902, -0.9068],
        [-1.1034, -2.0620],
        [-0.8758, -3.2173],
        [-2.2458, -0.9068],
        [-1.7825, -2.0620],
        [-1.4147, -3.2173],
        [-2.4615, -2.0620],
        [-1.9537, -3.2173],
        [-2.4926, -3.2173],
        [ 2.2063, -3.2494],
        [ 2.6468, -0.9389],
        [ 2.1008, -2.0942],
        [ 1.6674, -3.2494],
        [ 1.7913, -0.9389],
        [ 1.4217, -2.0942],
        [ 1.1284, -3.2494],
        [ 0.9357, -0.9389],
        [ 0.7427, -2.0942],
        [ 0.5895, -3.2494],
        [ 0.0802, -0.9389],
        [ 0.0637, -2.0942],
        [ 0.0505, -3.2494],
        [-0.7753, -0.9389],
        [-0.6154, -2.0942],
        [-0.4884, -3.2494],
        [-1.6308, -0.9389],
        [-1.2944, -2.0942],
        [-1.0274, -3.2494],
        [-2.4864, -0.9389],
        [-1.9734, -2.0942],
        [-1.5663, -3.2494],
        [-2.6525, -2.0942],
        [-2.1053, -3.2494],
        [ 2.6100, -2.0301],
        [ 2.0716, -3.1853],
        [ 2.4329, -0.8748],
        [ 1.9310, -2.0301],
        [ 1.5326, -3.1853],
        [ 1.5774, -0.8748],
        [ 1.2520, -2.0301],
        [ 0.9937, -3.1853],
        [ 0.7218, -0.8748],
        [ 0.5729, -2.0301],
        [ 0.4547, -3.1853],
        [-0.1337, -0.8748],
        [-0.1061, -2.0301],
        [-0.0842, -3.1853],
        [-0.9892, -0.8748],
        [-0.7851, -2.0301],
        [-0.6232, -3.1853],
        [-1.8447, -0.8748],
        [-1.4642, -2.0301],
        [-1.1621, -3.1853],
        [-2.7003, -0.8748],
        [-2.1432, -2.0301],
        [-1.7011, -3.1853],
        [-2.2400, -3.1853],
        [ 2.4758, -3.2494],
        [ 2.4403, -2.0942],
        [ 1.9368, -3.2494],
        [ 2.2190, -0.9389],
        [ 1.7612, -2.0942],
        [ 1.3979, -3.2494],
        [ 1.3635, -0.9389],
        [ 1.0822, -2.0942],
        [ 0.8589, -3.2494],
        [ 0.5080, -0.9389],
        [ 0.4032, -2.0942],
        [ 0.3200, -3.2494],
        [-0.3476, -0.9389],
        [-0.2759, -2.0942],
        [-0.2189, -3.2494],
        [-1.2031, -0.9389],
        [-0.9549, -2.0942],
        [-0.7579, -3.2494],
        [-2.0586, -0.9389],
        [-1.6339, -2.0942],
        [-1.2968, -3.2494],
        [-2.9141, -0.9389],
        [-2.3130, -2.0942],
        [-1.8358, -3.2494],
        [-2.3747, -3.2494],
        [ 2.3242, -3.2173],
        [ 2.8339, -0.9068],
        [ 2.2493, -2.0620],
        [ 1.7853, -3.2173],
        [ 1.9784, -0.9068],
        [ 1.5703, -2.0620],
        [ 1.2463, -3.2173],
        [ 1.1229, -0.9068],
        [ 0.8912, -2.0620],
        [ 0.7074, -3.2173],
        [ 0.2674, -0.9068],
        [ 0.2122, -2.0620],
        [ 0.1684, -3.2173],
        [-0.5882, -0.9068],
        [-0.4668, -2.0620],
        [-0.3705, -3.2173],
        [-1.4437, -0.9068],
        [-1.1459, -2.0620],
        [-0.9095, -3.2173],
        [-2.2992, -0.9068],
        [-1.8249, -2.0620],
        [-1.4484, -3.2173],
        [-2.5039, -2.0620],
        [-1.9874, -3.2173],
        [-2.5263, -3.2173],
        [ 2.7373, -2.0942],
        [ 2.1726, -3.2494],
        [ 2.5933, -0.9389],
        [ 2.0583, -2.0942],
        [ 1.6337, -3.2494],
        [ 1.7378, -0.9389],
        [ 1.3793, -2.0942],
        [ 1.0947, -3.2494],
        [ 0.8823, -0.9389],
        [ 0.7003, -2.0942],
        [ 0.5558, -3.2494],
        [ 0.0267, -0.9389],
        [ 0.0212, -2.0942],
        [ 0.0168, -3.2494],
        [-0.8288, -0.9389],
        [-0.6578, -2.0942],
        [-0.5221, -3.2494],
        [-1.6843, -0.9389],
        [-1.3368, -2.0942],
        [-1.0611, -3.2494],
        [-2.5398, -0.9389],
        [-2.0159, -2.0942],
        [-1.6000, -3.2494],
        [-2.6949, -2.0942],
        [-2.1389, -3.2494],
        [ 2.5251, -2.0942],
        [ 2.0042, -3.2494],
        [ 2.3260, -0.9389],
        [ 1.8461, -2.0942],
        [ 1.4653, -3.2494],
        [ 1.4704, -0.9389],
        [ 1.1671, -2.0942],
        [ 0.9263, -3.2494],
        [ 0.6149, -0.9389],
        [ 0.4881, -2.0942],
        [ 0.3874, -3.2494],
        [-0.2406, -0.9389],
        [-0.1910, -2.0942],
        [-0.1516, -3.2494],
        [-1.0961, -0.9389],
        [-0.8700, -2.0942],
        [-0.6905, -3.2494],
        [-1.9517, -0.9389],
        [-1.5490, -2.0942],
        [-1.2295, -3.2494],
        [-2.8072, -0.9389],
        [-2.2281, -2.0942],
        [-1.7684, -3.2494],
        [-2.3074, -3.2494],
        [ 2.3916, -3.2173],
        [ 2.9409, -0.9068],
        [ 2.3342, -2.0620],
        [ 1.8526, -3.2173],
        [ 2.0853, -0.9068],
        [ 1.6551, -2.0620],
        [ 1.3137, -3.2173],
        [ 1.2298, -0.9068],
        [ 0.9761, -2.0620],
        [ 0.7747, -3.2173],
        [ 0.3743, -0.9068],
        [ 0.2971, -2.0620],
        [ 0.2358, -3.2173],
        [-0.4812, -0.9068],
        [-0.3820, -2.0620],
        [-0.3032, -3.2173],
        [-1.3368, -0.9068],
        [-1.0610, -2.0620],
        [-0.8421, -3.2173],
        [-2.1923, -0.9068],
        [-1.7400, -2.0620],
        [-1.3811, -3.2173],
        [-3.0478, -0.9068],
        [-2.4190, -2.0620],
        [-1.9200, -3.2173],
        [-2.4589, -3.2173],
        [ 2.2737, -3.1853],
        [ 2.7537, -0.8748],
        [ 2.1856, -2.0301],
        [ 1.7347, -3.1853],
        [ 1.8982, -0.8748],
        [ 1.5066, -2.0301],
        [ 1.1958, -3.1853],
        [ 1.0427, -0.8748],
        [ 0.8276, -2.0301],
        [ 0.6568, -3.1853],
        [ 0.1871, -0.8748],
        [ 0.1485, -2.0301],
        [ 0.1179, -3.1853],
        [-0.6684, -0.8748],
        [-0.5305, -2.0301],
        [-0.4211, -3.1853],
        [-1.5239, -0.8748],
        [-1.2095, -2.0301],
        [-0.9600, -3.1853],
        [-2.3794, -0.8748],
        [-1.8886, -2.0301],
        [-1.4989, -3.1853],
        [-2.5676, -2.0301],
        [-2.0379, -3.1853],
        [ 2.6949, -2.0942],
        [ 2.1389, -3.2494],
        [ 2.5398, -0.9389],
        [ 2.0159, -2.0942],
        [ 1.6000, -3.2494],
        [ 1.6843, -0.9389],
        [ 1.3368, -2.0942],
        [ 1.0611, -3.2494],
        [ 0.8288, -0.9389],
        [ 0.6578, -2.0942],
        [ 0.5221, -3.2494],
        [-0.0267, -0.9389],
        [-0.0212, -2.0942],
        [-0.0168, -3.2494],
        [-0.8823, -0.9389],
        [-0.7003, -2.0942],
        [-0.5558, -3.2494],
        [-1.7378, -0.9389],
        [-1.3793, -2.0942],
        [-1.0947, -3.2494],
        [-2.5933, -0.9389],
        [-2.0583, -2.0942],
        [-1.6337, -3.2494],
        [-2.7373, -2.0942],
        [-2.1726, -3.2494],
        [ 2.5432, -3.1853],
        [ 2.5251, -2.0301],
        [ 2.0042, -3.1853],
        [ 2.3260, -0.8748],
        [ 1.8461, -2.0301],
        [ 1.4653, -3.1853],
        [ 1.4704, -0.8748],
        [ 1.1671, -2.0301],
        [ 0.9263, -3.1853],
        [ 0.6149, -0.8748],
        [ 0.4881, -2.0301],
        [ 0.3874, -3.1853],
        [-0.2406, -0.8748],
        [-0.1910, -2.0301],
        [-0.1516, -3.1853],
        [-1.0961, -0.8748],
        [-0.8700, -2.0301],
        [-0.6905, -3.1853],
        [-1.9517, -0.8748],
        [-1.5490, -2.0301],
        [-1.2295, -3.1853],
        [-2.8072, -0.8748],
        [-2.2281, -2.0301],
        [-1.7684, -3.1853],
        [-2.3074, -3.1853],
        [ 2.4084, -3.2494],
        [ 2.9676, -0.9389],
        [ 2.3554, -2.0942],
        [ 1.8695, -3.2494],
        [ 2.1121, -0.9389],
        [ 1.6764, -2.0942],
        [ 1.3305, -3.2494],
        [ 1.2566, -0.9389],
        [ 0.9973, -2.0942],
        [ 0.7916, -3.2494],
        [ 0.4010, -0.9389],
        [ 0.3183, -2.0942],
        [ 0.2526, -3.2494],
        [-0.4545, -0.9389],
        [-0.3607, -2.0942],
        [-0.2863, -3.2494],
        [-1.3100, -0.9389],
        [-1.0398, -2.0942],
        [-0.8253, -3.2494],
        [-2.1655, -0.9389],
        [-1.7188, -2.0942],
        [-1.3642, -3.2494],
        [-3.0211, -0.9389],
        [-2.3978, -2.0942],
        [-1.9032, -3.2494],
        [-2.4421, -3.2494]])
epoch: 0, iter: 0 jth_regression_loss in regressionLoss forward
 tensor([[2.1676e+00, 3.1617e+00],
        [2.6180e+00, 8.5123e-01],
        [2.0664e+00, 2.0065e+00],
        [1.6287e+00, 3.1617e+00],
        [1.7624e+00, 8.5123e-01],
        [1.3874e+00, 2.0065e+00],
        [1.0897e+00, 3.1617e+00],
        [9.0691e-01, 8.5123e-01],
        [7.0836e-01, 2.0065e+00],
        [5.5076e-01, 3.1617e+00],
        [5.1463e-02, 8.5123e-01],
        [3.2420e-02, 2.0065e+00],
        [2.0423e-02, 3.1617e+00],
        [6.9303e-01, 8.5123e-01],
        [5.3860e-01, 2.0065e+00],
        [4.1602e-01, 3.1617e+00],
        [1.5486e+00, 8.5123e-01],
        [1.2176e+00, 2.0065e+00],
        [9.5497e-01, 3.1617e+00],
        [2.4041e+00, 8.5123e-01],
        [1.8967e+00, 2.0065e+00],
        [1.4939e+00, 3.1617e+00],
        [2.5757e+00, 2.0065e+00],
        [2.0329e+00, 3.1617e+00],
        [2.5969e+00, 1.9745e+00],
        [2.0497e+00, 3.1298e+00],
        [2.4308e+00, 8.1928e-01],
        [1.9179e+00, 1.9745e+00],
        [1.5108e+00, 3.1298e+00],
        [1.5753e+00, 8.1928e-01],
        [1.2388e+00, 1.9745e+00],
        [9.7181e-01, 3.1298e+00],
        [7.1976e-01, 8.1928e-01],
        [5.5982e-01, 1.9745e+00],
        [4.3287e-01, 3.1298e+00],
        [2.8948e-02, 8.1928e-01],
        [1.8236e-02, 1.9745e+00],
        [1.1488e-02, 3.1298e+00],
        [8.8018e-01, 8.1928e-01],
        [6.8714e-01, 1.9745e+00],
        [5.3392e-01, 3.1298e+00],
        [1.7357e+00, 8.1928e-01],
        [1.3662e+00, 1.9745e+00],
        [1.0729e+00, 3.1298e+00],
        [2.5912e+00, 8.1928e-01],
        [2.0452e+00, 1.9745e+00],
        [1.6118e+00, 3.1298e+00],
        [2.1508e+00, 3.1298e+00],
        [2.4539e+00, 3.1939e+00],
        [2.4272e+00, 2.0386e+00],
        [1.9150e+00, 3.1939e+00],
        [2.2169e+00, 8.8338e-01],
        [1.7481e+00, 2.0386e+00],
        [1.3760e+00, 3.1939e+00],
        [1.3614e+00, 8.8338e-01],
        [1.0691e+00, 2.0386e+00],
        [8.3708e-01, 3.1939e+00],
        [5.0588e-01, 8.8338e-01],
        [3.9006e-01, 2.0386e+00],
        [2.9813e-01, 3.1939e+00],
        [2.3853e-01, 8.8338e-01],
        [1.7786e-01, 2.0386e+00],
        [1.2971e-01, 3.1939e+00],
        [1.0941e+00, 8.8338e-01],
        [8.5689e-01, 2.0386e+00],
        [6.6866e-01, 3.1939e+00],
        [1.9496e+00, 8.8338e-01],
        [1.5359e+00, 2.0386e+00],
        [1.2076e+00, 3.1939e+00],
        [2.8051e+00, 8.8338e-01],
        [2.2150e+00, 2.0386e+00],
        [1.7466e+00, 3.1939e+00],
        [2.2855e+00, 3.1939e+00],
        [2.3023e+00, 3.1617e+00],
        [2.8318e+00, 8.5123e-01],
        [2.2362e+00, 2.0065e+00],
        [1.7634e+00, 3.1617e+00],
        [1.9763e+00, 8.5123e-01],
        [1.5571e+00, 2.0065e+00],
        [1.2244e+00, 3.1617e+00],
        [1.1208e+00, 8.5123e-01],
        [8.7811e-01, 2.0065e+00],
        [6.8550e-01, 3.1617e+00],
        [2.6527e-01, 8.5123e-01],
        [1.9908e-01, 2.0065e+00],
        [1.4655e-01, 3.1617e+00],
        [4.7915e-01, 8.5123e-01],
        [3.6884e-01, 2.0065e+00],
        [2.8129e-01, 3.1617e+00],
        [1.3347e+00, 8.5123e-01],
        [1.0479e+00, 2.0065e+00],
        [8.2024e-01, 3.1617e+00],
        [2.1902e+00, 8.5123e-01],
        [1.7269e+00, 2.0065e+00],
        [1.3592e+00, 3.1617e+00],
        [2.4059e+00, 2.0065e+00],
        [1.8981e+00, 3.1617e+00],
        [2.4371e+00, 3.1617e+00],
        [2.1508e+00, 3.1939e+00],
        [2.5912e+00, 8.8338e-01],
        [2.0452e+00, 2.0386e+00],
        [1.6118e+00, 3.1939e+00],
        [1.7357e+00, 8.8338e-01],
        [1.3662e+00, 2.0386e+00],
        [1.0729e+00, 3.1939e+00],
        [8.8018e-01, 8.8338e-01],
        [6.8714e-01, 2.0386e+00],
        [5.3392e-01, 3.1939e+00],
        [2.8948e-02, 8.8338e-01],
        [1.8236e-02, 2.0386e+00],
        [1.1488e-02, 3.1939e+00],
        [7.1976e-01, 8.8338e-01],
        [5.5982e-01, 2.0386e+00],
        [4.3287e-01, 3.1939e+00],
        [1.5753e+00, 8.8338e-01],
        [1.2388e+00, 2.0386e+00],
        [9.7181e-01, 3.1939e+00],
        [2.4308e+00, 8.8338e-01],
        [1.9179e+00, 2.0386e+00],
        [1.5108e+00, 3.1939e+00],
        [2.5969e+00, 2.0386e+00],
        [2.0497e+00, 3.1939e+00],
        [2.5545e+00, 1.9745e+00],
        [2.0160e+00, 3.1298e+00],
        [2.3773e+00, 8.1928e-01],
        [1.8754e+00, 1.9745e+00],
        [1.4771e+00, 3.1298e+00],
        [1.5218e+00, 8.1928e-01],
        [1.1964e+00, 1.9745e+00],
        [9.3813e-01, 3.1298e+00],
        [6.6629e-01, 8.1928e-01],
        [5.1738e-01, 1.9745e+00],
        [3.9918e-01, 3.1298e+00],
        [7.8120e-02, 8.1928e-01],
        [5.0656e-02, 1.9745e+00],
        [3.1911e-02, 3.1298e+00],
        [9.3365e-01, 8.1928e-01],
        [7.2958e-01, 1.9745e+00],
        [5.6760e-01, 3.1298e+00],
        [1.7892e+00, 8.1928e-01],
        [1.4086e+00, 1.9745e+00],
        [1.1066e+00, 3.1298e+00],
        [2.6447e+00, 8.1928e-01],
        [2.0876e+00, 1.9745e+00],
        [1.6455e+00, 3.1298e+00],
        [2.1844e+00, 3.1298e+00],
        [2.4202e+00, 3.1939e+00],
        [2.3847e+00, 2.0386e+00],
        [1.8813e+00, 3.1939e+00],
        [2.1635e+00, 8.8338e-01],
        [1.7057e+00, 2.0386e+00],
        [1.3423e+00, 3.1939e+00],
        [1.3079e+00, 8.8338e-01],
        [1.0267e+00, 2.0386e+00],
        [8.0339e-01, 3.1939e+00],
        [4.5241e-01, 8.8338e-01],
        [3.4762e-01, 2.0386e+00],
        [2.6444e-01, 3.1939e+00],
        [2.9200e-01, 8.8338e-01],
        [2.2030e-01, 2.0386e+00],
        [1.6339e-01, 3.1939e+00],
        [1.1475e+00, 8.8338e-01],
        [8.9933e-01, 2.0386e+00],
        [7.0234e-01, 3.1939e+00],
        [2.0031e+00, 8.8338e-01],
        [1.5784e+00, 2.0386e+00],
        [1.2413e+00, 3.1939e+00],
        [2.8586e+00, 8.8338e-01],
        [2.2574e+00, 2.0386e+00],
        [1.7802e+00, 3.1939e+00],
        [2.3192e+00, 3.1939e+00],
        [2.2687e+00, 3.1617e+00],
        [2.7784e+00, 8.5123e-01],
        [2.1937e+00, 2.0065e+00],
        [1.7297e+00, 3.1617e+00],
        [1.9228e+00, 8.5123e-01],
        [1.5147e+00, 2.0065e+00],
        [1.1908e+00, 3.1617e+00],
        [1.0673e+00, 8.5123e-01],
        [8.3567e-01, 2.0065e+00],
        [6.5181e-01, 3.1617e+00],
        [2.1180e-01, 8.5123e-01],
        [1.5664e-01, 2.0065e+00],
        [1.1287e-01, 3.1617e+00],
        [5.3262e-01, 8.5123e-01],
        [4.1128e-01, 2.0065e+00],
        [3.1497e-01, 3.1617e+00],
        [1.3881e+00, 8.5123e-01],
        [1.0903e+00, 2.0065e+00],
        [8.5392e-01, 3.1617e+00],
        [2.2437e+00, 8.5123e-01],
        [1.7693e+00, 2.0065e+00],
        [1.3929e+00, 3.1617e+00],
        [2.4484e+00, 2.0065e+00],
        [1.9318e+00, 3.1617e+00],
        [2.4708e+00, 3.1617e+00],
        [2.6818e+00, 2.0386e+00],
        [2.1171e+00, 3.1939e+00],
        [2.5378e+00, 8.8338e-01],
        [2.0028e+00, 2.0386e+00],
        [1.5781e+00, 3.1939e+00],
        [1.6822e+00, 8.8338e-01],
        [1.3237e+00, 2.0386e+00],
        [1.0392e+00, 3.1939e+00],
        [8.2671e-01, 8.8338e-01],
        [6.4470e-01, 2.0386e+00],
        [5.0023e-01, 3.1939e+00],
        [3.2165e-03, 8.8338e-01],
        [2.0262e-03, 2.0386e+00],
        [1.2765e-03, 3.1939e+00],
        [7.7323e-01, 8.8338e-01],
        [6.0226e-01, 2.0386e+00],
        [4.6655e-01, 3.1939e+00],
        [1.6288e+00, 8.8338e-01],
        [1.2813e+00, 2.0386e+00],
        [1.0055e+00, 3.1939e+00],
        [2.4843e+00, 8.8338e-01],
        [1.9603e+00, 2.0386e+00],
        [1.5444e+00, 3.1939e+00],
        [2.6393e+00, 2.0386e+00],
        [2.0834e+00, 3.1939e+00],
        [2.4696e+00, 2.0386e+00],
        [1.9487e+00, 3.1939e+00],
        [2.2704e+00, 8.8338e-01],
        [1.7906e+00, 2.0386e+00],
        [1.4097e+00, 3.1939e+00],
        [1.4149e+00, 8.8338e-01],
        [1.1115e+00, 2.0386e+00],
        [8.7076e-01, 3.1939e+00],
        [5.5935e-01, 8.8338e-01],
        [4.3250e-01, 2.0386e+00],
        [3.3181e-01, 3.1939e+00],
        [1.8506e-01, 8.8338e-01],
        [1.3542e-01, 2.0386e+00],
        [9.6023e-02, 3.1939e+00],
        [1.0406e+00, 8.8338e-01],
        [8.1445e-01, 2.0386e+00],
        [6.3497e-01, 3.1939e+00],
        [1.8961e+00, 8.8338e-01],
        [1.4935e+00, 2.0386e+00],
        [1.1739e+00, 3.1939e+00],
        [2.7516e+00, 8.8338e-01],
        [2.1725e+00, 2.0386e+00],
        [1.7129e+00, 3.1939e+00],
        [2.2518e+00, 3.1939e+00],
        [2.3360e+00, 3.1617e+00],
        [2.8853e+00, 8.5123e-01],
        [2.2786e+00, 2.0065e+00],
        [1.7971e+00, 3.1617e+00],
        [2.0298e+00, 8.5123e-01],
        [1.5996e+00, 2.0065e+00],
        [1.2581e+00, 3.1617e+00],
        [1.1743e+00, 8.5123e-01],
        [9.2055e-01, 2.0065e+00],
        [7.1918e-01, 3.1617e+00],
        [3.1874e-01, 8.5123e-01],
        [2.4152e-01, 2.0065e+00],
        [1.8023e-01, 3.1617e+00],
        [4.2568e-01, 8.5123e-01],
        [3.2640e-01, 2.0065e+00],
        [2.4760e-01, 3.1617e+00],
        [1.2812e+00, 8.5123e-01],
        [1.0054e+00, 2.0065e+00],
        [7.8655e-01, 3.1617e+00],
        [2.1367e+00, 8.5123e-01],
        [1.6845e+00, 2.0065e+00],
        [1.3255e+00, 3.1617e+00],
        [2.9923e+00, 8.5123e-01],
        [2.3635e+00, 2.0065e+00],
        [1.8644e+00, 3.1617e+00],
        [2.4034e+00, 3.1617e+00],
        [2.2181e+00, 3.1298e+00],
        [2.6982e+00, 8.1928e-01],
        [2.1301e+00, 1.9745e+00],
        [1.6792e+00, 3.1298e+00],
        [1.8426e+00, 8.1928e-01],
        [1.4510e+00, 1.9745e+00],
        [1.1402e+00, 3.1298e+00],
        [9.8712e-01, 8.1928e-01],
        [7.7201e-01, 1.9745e+00],
        [6.0129e-01, 3.1298e+00],
        [1.3159e-01, 8.1928e-01],
        [9.2982e-02, 1.9745e+00],
        [6.2339e-02, 3.1298e+00],
        [6.1282e-01, 8.1928e-01],
        [4.7494e-01, 1.9745e+00],
        [3.6550e-01, 3.1298e+00],
        [1.4683e+00, 8.1928e-01],
        [1.1540e+00, 1.9745e+00],
        [9.0444e-01, 3.1298e+00],
        [2.3239e+00, 8.1928e-01],
        [1.8330e+00, 1.9745e+00],
        [1.4434e+00, 3.1298e+00],
        [2.5120e+00, 1.9745e+00],
        [1.9823e+00, 3.1298e+00],
        [2.6393e+00, 2.0386e+00],
        [2.0834e+00, 3.1939e+00],
        [2.4843e+00, 8.8338e-01],
        [1.9603e+00, 2.0386e+00],
        [1.5444e+00, 3.1939e+00],
        [1.6288e+00, 8.8338e-01],
        [1.2813e+00, 2.0386e+00],
        [1.0055e+00, 3.1939e+00],
        [7.7323e-01, 8.8338e-01],
        [6.0226e-01, 2.0386e+00],
        [4.6655e-01, 3.1939e+00],
        [3.2165e-03, 8.8338e-01],
        [2.0262e-03, 2.0386e+00],
        [1.2765e-03, 3.1939e+00],
        [8.2671e-01, 8.8338e-01],
        [6.4470e-01, 2.0386e+00],
        [5.0023e-01, 3.1939e+00],
        [1.6822e+00, 8.8338e-01],
        [1.3237e+00, 2.0386e+00],
        [1.0392e+00, 3.1939e+00],
        [2.5378e+00, 8.8338e-01],
        [2.0028e+00, 2.0386e+00],
        [1.5781e+00, 3.1939e+00],
        [2.6818e+00, 2.0386e+00],
        [2.1171e+00, 3.1939e+00],
        [2.4876e+00, 3.1298e+00],
        [2.4696e+00, 1.9745e+00],
        [1.9487e+00, 3.1298e+00],
        [2.2704e+00, 8.1928e-01],
        [1.7906e+00, 1.9745e+00],
        [1.4097e+00, 3.1298e+00],
        [1.4149e+00, 8.1928e-01],
        [1.1115e+00, 1.9745e+00],
        [8.7076e-01, 3.1298e+00],
        [5.5935e-01, 8.1928e-01],
        [4.3250e-01, 1.9745e+00],
        [3.3181e-01, 3.1298e+00],
        [1.8506e-01, 8.1928e-01],
        [1.3542e-01, 1.9745e+00],
        [9.6023e-02, 3.1298e+00],
        [1.0406e+00, 8.1928e-01],
        [8.1445e-01, 1.9745e+00],
        [6.3497e-01, 3.1298e+00],
        [1.8961e+00, 8.1928e-01],
        [1.4935e+00, 1.9745e+00],
        [1.1739e+00, 3.1298e+00],
        [2.7516e+00, 8.1928e-01],
        [2.1725e+00, 1.9745e+00],
        [1.7129e+00, 3.1298e+00],
        [2.2518e+00, 3.1298e+00],
        [2.3529e+00, 3.1939e+00],
        [2.9120e+00, 8.8338e-01],
        [2.2998e+00, 2.0386e+00],
        [1.8139e+00, 3.1939e+00],
        [2.0565e+00, 8.8338e-01],
        [1.6208e+00, 2.0386e+00],
        [1.2750e+00, 3.1939e+00],
        [1.2010e+00, 8.8338e-01],
        [9.4177e-01, 2.0386e+00],
        [7.3602e-01, 3.1939e+00],
        [3.4547e-01, 8.8338e-01],
        [2.6274e-01, 2.0386e+00],
        [1.9708e-01, 3.1939e+00],
        [3.9894e-01, 8.8338e-01],
        [3.0518e-01, 2.0386e+00],
        [2.3076e-01, 3.1939e+00],
        [1.2545e+00, 8.8338e-01],
        [9.8421e-01, 2.0386e+00],
        [7.6971e-01, 3.1939e+00],
        [2.1100e+00, 8.8338e-01],
        [1.6632e+00, 2.0386e+00],
        [1.3087e+00, 3.1939e+00],
        [2.9655e+00, 8.8338e-01],
        [2.3423e+00, 2.0386e+00],
        [1.8476e+00, 3.1939e+00],
        [2.3865e+00, 3.1939e+00]], grad_fn=<SWhereBackward0>)
feature index 4
cls loss:
 tensor([1.1284], grad_fn=<MeanBackward1>)
 reg loss: tensor([1.5740], grad_fn=<MeanBackward1>)

focal_losses in ResNet forward (before stack and mean):
[tensor([1.1284], grad_fn=<MeanBackward1>), tensor([1.1284], grad_fn=<MeanBackward1>), tensor([1.1284], grad_fn=<MeanBackward1>), tensor([1.1284], grad_fn=<MeanBackward1>), tensor([1.1284], grad_fn=<MeanBackward1>)]
regression_losses in ResNet forward (before stack and mean):
[tensor([1.6554], grad_fn=<MeanBackward1>), tensor([1.3746], grad_fn=<MeanBackward1>), tensor([1.6491], grad_fn=<MeanBackward1>), tensor([1.5523], grad_fn=<MeanBackward1>), tensor([1.5740], grad_fn=<MeanBackward1>)]
focal_loss in ResNet forward (after stack and mean):
tensor([[1.1284]], grad_fn=<MeanBackward1>)
regression_loss in ResNet forward (after stack and mean):
tensor([[1.5611]], grad_fn=<MeanBackward1>)
cls loss in train loop (before mean):
tensor([[1.1284]], grad_fn=<MeanBackward1>)
reg loss in train loop (before mean):
tensor([[1.5611]], grad_fn=<MeanBackward1>)
cls loss in train loop (after mean):
1.1283884048461914
reg loss in train loop (after mean):
1.5610558986663818
total loss in train loop:
2.6894443035125732
^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^Czsh: segmentation fault  python train.py --ballroom_audio_dir  --ballroom_annot_dir  --preload  10    
[1m[7m%[27m[1m[0m                                                                                                                                                                                                                    ]7;file://anjaehuns-MacBook-Pro.local/Volumes/External%201/GitHub/beat-retinanet[0m[27m[24m[J(base) jaehoon@anjaehuns-MacBook-Pro beat-retinanet % [K[?2004h/opt/miniconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[?2004l
[1m[7m%[27m[1m[0m                                                                                                                                                                                                                    ]7;file://anjaehuns-MacBook-Pro.local/Volumes/External%201/GitHub/beat-retinanet[0m[27m[24m[J(base) jaehoon@anjaehuns-MacBook-Pro beat-retinanet % [K[?2004hlls[?2004l
LICENSE				checkpoints_2			k-means.py			output.txt			top_level_preds.txt		visualize_single_image.py
README.md			checkpoints_old			k-means2.py			recent_checkpoints		top_level_targets.txt
anchors.txt			coco_validation.py		k-means3.py			results				train.py
beat_validation.py		csv_validation.py		log.log				retinanet			train_old.py
checkpoints			images				logfile.txt			save				visualize.py
[1m[7m%[27m[1m[0m                                                                                                                                                                                                                    ]7;file://anjaehuns-MacBook-Pro.local/Volumes/External%201/GitHub/beat-retinanet[0m[27m[24m[J(base) jaehoon@anjaehuns-MacBook-Pro beat-retinanet % [K[?2004hc c 